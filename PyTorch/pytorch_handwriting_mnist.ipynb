{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_handwriting_mnist.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNVhyM1VOZpwh+plxBWbhUq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaashaJoshi/tf-keras-pytorch-examples/blob/master/PyTorch/pytorch_handwriting_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjWXKIIEjaFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-cwSi4EjzGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform = transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)    # Helps in iterating the data using iter(trainloader)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvya_O3FkTti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8d675436-7cf4-4ec4-ae7e-d26ed9ad21f1"
      },
      "source": [
        "data = iter(trainloader)\n",
        "images, labels = data.next()\n",
        "print(type(images))\n",
        "print(images.shape) # Batch = 64 images (28, 28, 1) i.e. grayscale images\n",
        "print(labels.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILbbFw3okpoU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "cc84053d-2977-416c-92ea-866fd6e0277b"
      },
      "source": [
        "plt.imshow(images[0].numpy().squeeze(), cmap = 'gray')     # .squeeze returns a tensor with all the dimensions of input of size 1 removed. input: (1, 28, 28) output: (28, 28)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f86e5968470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwZlURV7pv-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_activation(x):\n",
        "  return 1 /(1 + torch.exp(-x))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV6RieT1v1BW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Neural Network Architecture\n",
        "\n",
        "Input Layer\n",
        "inputs: (64, 784) [batch_size = 64]\n",
        "\n",
        "Hidden Layer 1\n",
        "inputs = inputs: (64, 784)\n",
        "weight1: (input_size, output_size) = (784, 256)\n",
        "bias1: (input_size) = (256)\n",
        "output_hidden_layer1 (h1): (64, 256) [batch_size = 64]\n",
        "\n",
        "** output = actvation(torch.mm(inputs, weights) + bias) **\n",
        "\n",
        "# Check why not torch.mm(weights, inputs) as in the theoretical formula!!\n",
        "\n",
        "Output Layer\n",
        "inputs = output of hidden layer 1: (64, 256)\n",
        "weight2: (256, 10) [10 outputs to classify numbers from 0 to 9]\n",
        "bias2: (10)\n",
        "final_output (output): (64, 10) [batch_size = 64]\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC-woIMelbnB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53e747aa-2f4c-45f8-bcdf-b396f8ecc82b"
      },
      "source": [
        "# Flatten the input (images) to dimension of (64, 784)\n",
        "\n",
        "inputs = images.view(images.shape[0], -1)   # .view reshapes the images to (64, 784). Refer below.\n",
        "print(inputs.shape)\n",
        "\n",
        "# images.shape[0] gievs the batch_size"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG_jSpixxf9T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0da11b22-73ff-46a8-822b-35a65fceb447"
      },
      "source": [
        "'''\n",
        "Reshape functions and their working\n",
        "\n",
        "img = torch.randn(64, 28, 28)\n",
        "print(img.shape)\n",
        "\n",
        "# img = img.reshape(64, -1)\n",
        "# img.resize_(64, -1)\n",
        "img = img.view(64, -1)\n",
        "print(img.shape)    # All 3 variants produce (64, 784)\n",
        "'''"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nReshape functions and their working\\n\\nimg = torch.randn(64, 28, 28)\\nprint(img.shape)\\n\\n# img = img.reshape(64, -1)\\n# img.resize_(64, -1)\\nimg = img.view(64, -1)\\nprint(img.shape)    # All 3 variants produce (64, 784)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhPG7TDsni9w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fbec5a80-560f-44f8-e18b-9327cc1f2532"
      },
      "source": [
        "# weights W(input_size, hidden_size) and bias B(hidden_size, output_size)\n",
        "W1 = torch.randn(784, 256)\n",
        "B1 = torch.randn(256)     # Check bias = torch.randn(256, 64)!\n",
        "# Error: The size of tensor a (256) must match the size of tensor b (64) at non-singleton dimension 1.\n",
        "\n",
        "W2 = torch.randn(256, 10)\n",
        "B2 = torch.randn(10)\n",
        "\n",
        "h1 = sigmoid_activation(torch.mm(inputs, W1) + B1)\n",
        "output = sigmoid_activation(torch.mm(h1, W2) + B2)\n",
        "print(output)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[9.7746e-07, 9.0075e-06, 1.7139e-06, 2.0623e-04, 9.9999e-01, 9.9880e-01,\n",
            "         2.3687e-05, 8.9603e-01, 1.0000e+00, 4.7987e-06],\n",
            "        [1.6461e-03, 2.2016e-02, 1.9584e-09, 5.0734e-01, 9.9987e-01, 9.9903e-01,\n",
            "         2.6726e-03, 3.2245e-03, 9.9838e-01, 4.5940e-06],\n",
            "        [8.1079e-06, 4.9693e-06, 1.3090e-08, 1.0341e-06, 1.0000e+00, 1.0000e+00,\n",
            "         1.6189e-06, 2.3561e-01, 1.0000e+00, 4.4318e-06],\n",
            "        [1.2884e-08, 3.4453e-01, 3.4516e-05, 1.2746e-03, 9.7622e-01, 9.9451e-01,\n",
            "         6.5933e-01, 9.9733e-01, 1.0000e+00, 1.3656e-07],\n",
            "        [5.1864e-01, 9.9970e-01, 5.2125e-11, 8.8268e-01, 9.9956e-01, 1.0000e+00,\n",
            "         4.0783e-03, 1.3839e-04, 1.0000e+00, 2.3973e-05],\n",
            "        [5.4091e-05, 6.0377e-04, 4.1546e-08, 2.4275e-04, 1.0000e+00, 1.0000e+00,\n",
            "         2.7375e-07, 3.9004e-01, 1.0000e+00, 3.2752e-09],\n",
            "        [1.1096e-06, 7.1171e-04, 8.1656e-13, 1.1496e-01, 9.9991e-01, 1.0000e+00,\n",
            "         2.2206e-09, 9.3480e-04, 1.0000e+00, 2.5389e-10],\n",
            "        [3.3981e-01, 9.3409e-01, 7.2767e-10, 7.6243e-01, 8.9725e-01, 1.0000e+00,\n",
            "         2.0489e-11, 9.7596e-06, 1.0000e+00, 4.7320e-06],\n",
            "        [5.4147e-08, 1.6686e-04, 8.6776e-12, 1.1680e-01, 9.9925e-01, 9.9988e-01,\n",
            "         1.5767e-01, 9.9999e-01, 9.9999e-01, 3.0433e-04],\n",
            "        [1.6990e-06, 2.2120e-01, 1.4529e-07, 9.7307e-01, 1.3577e-03, 1.0000e+00,\n",
            "         1.1619e-03, 9.9669e-01, 1.0000e+00, 5.9682e-06],\n",
            "        [1.1342e-06, 9.6251e-01, 9.6673e-11, 8.6326e-01, 1.0000e+00, 9.9294e-01,\n",
            "         1.8190e-06, 2.1205e-03, 1.0000e+00, 8.6090e-01],\n",
            "        [8.7970e-07, 9.9442e-01, 2.7125e-08, 3.3642e-03, 1.0000e+00, 9.9992e-01,\n",
            "         4.4728e-05, 9.3956e-01, 1.0000e+00, 6.7103e-12],\n",
            "        [4.9896e-06, 9.5359e-05, 1.1153e-06, 8.9184e-03, 1.0000e+00, 9.9889e-01,\n",
            "         1.5811e-09, 3.0203e-05, 5.6911e-01, 9.7048e-08],\n",
            "        [6.5682e-11, 2.4149e-01, 1.3144e-09, 9.9987e-01, 7.9905e-01, 1.0000e+00,\n",
            "         5.0644e-06, 2.8423e-03, 7.9820e-02, 7.6167e-01],\n",
            "        [2.5922e-04, 5.5962e-03, 9.5139e-14, 9.9998e-01, 9.9498e-01, 9.9993e-01,\n",
            "         1.5154e-03, 5.4594e-01, 1.0000e+00, 4.0908e-04],\n",
            "        [2.7397e-05, 6.9022e-03, 1.5568e-12, 3.5106e-02, 1.0000e+00, 1.0000e+00,\n",
            "         1.1727e-04, 3.0575e-02, 1.0000e+00, 8.2029e-09],\n",
            "        [2.2971e-07, 9.9801e-01, 1.1675e-08, 2.1329e-05, 9.9939e-01, 1.0000e+00,\n",
            "         1.4372e-04, 4.2258e-04, 1.0000e+00, 1.5858e-07],\n",
            "        [3.8785e-02, 1.8538e-03, 1.8703e-06, 3.6409e-04, 9.9999e-01, 1.0000e+00,\n",
            "         9.7604e-05, 5.6206e-02, 1.0000e+00, 3.4871e-10],\n",
            "        [7.4502e-04, 1.0000e+00, 1.8814e-12, 2.6400e-03, 1.0000e+00, 9.9999e-01,\n",
            "         5.6855e-05, 3.9220e-01, 9.9997e-01, 8.9274e-08],\n",
            "        [6.4271e-05, 3.1589e-04, 5.2909e-07, 3.9431e-01, 6.0885e-01, 1.0000e+00,\n",
            "         1.2861e-06, 5.0794e-01, 1.0000e+00, 8.0798e-11],\n",
            "        [7.9202e-05, 1.8476e-03, 4.6565e-10, 8.3634e-04, 1.0000e+00, 1.0000e+00,\n",
            "         3.3053e-05, 5.4598e-03, 1.0000e+00, 2.8483e-07],\n",
            "        [1.5185e-01, 3.0843e-08, 6.0769e-08, 7.2197e-03, 9.9994e-01, 1.0000e+00,\n",
            "         2.3749e-03, 1.5242e-01, 1.0000e+00, 1.0327e-06],\n",
            "        [1.3225e-06, 1.5072e-02, 2.1820e-06, 9.1966e-01, 9.9995e-01, 9.9970e-01,\n",
            "         1.0328e-05, 1.2890e-01, 9.9729e-01, 5.4185e-09],\n",
            "        [1.2723e-07, 9.2662e-01, 1.6762e-08, 9.9814e-01, 9.9939e-01, 1.0000e+00,\n",
            "         1.6849e-01, 1.3445e-06, 9.9988e-01, 1.8698e-02],\n",
            "        [1.8154e-04, 3.8375e-06, 7.1278e-08, 9.9945e-01, 9.7499e-01, 9.9985e-01,\n",
            "         1.5152e-12, 7.0699e-02, 1.0000e+00, 9.7639e-03],\n",
            "        [3.3506e-08, 1.3342e-04, 1.2358e-03, 9.9972e-01, 9.9548e-01, 9.9997e-01,\n",
            "         3.0445e-10, 1.3331e-04, 1.0000e+00, 4.9669e-05],\n",
            "        [7.2379e-04, 9.7564e-01, 2.9097e-08, 9.5741e-01, 1.0000e+00, 9.9883e-01,\n",
            "         2.6761e-07, 4.9684e-02, 9.9884e-01, 2.0922e-09],\n",
            "        [3.4144e-01, 8.7611e-04, 7.5784e-15, 9.8921e-01, 9.9922e-01, 9.9996e-01,\n",
            "         2.6923e-05, 9.9669e-01, 1.0000e+00, 2.0674e-09],\n",
            "        [6.9445e-05, 8.6588e-02, 6.8135e-04, 9.8847e-01, 1.0000e+00, 9.9601e-01,\n",
            "         6.3998e-01, 3.3554e-02, 1.0000e+00, 1.4594e-09],\n",
            "        [5.3057e-06, 1.0087e-07, 2.5599e-11, 9.1697e-02, 9.9997e-01, 1.0000e+00,\n",
            "         6.0231e-02, 8.1870e-01, 1.0000e+00, 1.7093e-02],\n",
            "        [2.9223e-08, 1.2923e-01, 8.4322e-14, 9.9275e-01, 1.0000e+00, 1.0000e+00,\n",
            "         3.7501e-09, 4.9830e-06, 9.9999e-01, 5.5575e-09],\n",
            "        [1.3989e-06, 9.9534e-01, 1.7111e-02, 8.4408e-01, 1.0000e+00, 9.9960e-01,\n",
            "         2.4857e-03, 1.1498e-02, 1.0000e+00, 1.6454e-05],\n",
            "        [8.7323e-04, 9.9420e-01, 5.4346e-08, 2.7402e-06, 1.0000e+00, 9.9937e-01,\n",
            "         3.6472e-06, 8.0515e-01, 1.0000e+00, 1.6076e-08],\n",
            "        [1.6472e-03, 9.7649e-06, 7.0834e-09, 2.4464e-01, 9.9945e-01, 9.9998e-01,\n",
            "         1.3506e-05, 3.3531e-04, 1.0000e+00, 5.2987e-07],\n",
            "        [2.8316e-08, 1.8012e-03, 1.3794e-06, 2.9229e-04, 9.9999e-01, 9.9445e-01,\n",
            "         1.4683e-06, 4.5601e-01, 1.0000e+00, 5.2329e-05],\n",
            "        [4.8783e-02, 8.6396e-03, 1.4697e-11, 3.0868e-02, 9.9968e-01, 1.0000e+00,\n",
            "         2.4818e-04, 6.2168e-06, 1.0000e+00, 2.5622e-09],\n",
            "        [1.1359e-02, 4.0213e-03, 3.8532e-10, 3.7693e-02, 9.9999e-01, 9.9997e-01,\n",
            "         1.5014e-01, 5.0032e-04, 1.0000e+00, 1.1281e-03],\n",
            "        [4.0880e-07, 2.8166e-02, 7.6934e-06, 3.3276e-02, 9.9998e-01, 1.0000e+00,\n",
            "         3.9820e-02, 9.6170e-01, 1.0000e+00, 1.8604e-07],\n",
            "        [4.8440e-09, 6.0372e-01, 1.5406e-07, 9.9859e-01, 9.9946e-01, 9.9825e-01,\n",
            "         3.8677e-03, 7.8820e-03, 1.0000e+00, 7.6039e-06],\n",
            "        [2.6036e-05, 2.5113e-04, 2.2198e-05, 9.6735e-01, 9.9987e-01, 1.0000e+00,\n",
            "         9.9989e-01, 1.0327e-02, 1.0000e+00, 1.1187e-08],\n",
            "        [1.3481e-01, 6.7732e-03, 1.8810e-07, 2.9610e-03, 9.9944e-01, 1.0000e+00,\n",
            "         9.0933e-05, 8.1870e-06, 1.0000e+00, 4.5630e-03],\n",
            "        [1.7612e-11, 9.5374e-01, 1.6671e-08, 4.4491e-01, 9.9996e-01, 9.9981e-01,\n",
            "         3.3313e-04, 6.5309e-01, 1.0000e+00, 3.2207e-01],\n",
            "        [9.4540e-01, 7.6051e-01, 1.1367e-07, 7.9636e-05, 9.9982e-01, 1.0000e+00,\n",
            "         3.2595e-06, 6.1969e-03, 1.0000e+00, 8.9111e-05],\n",
            "        [2.4536e-04, 9.9998e-01, 1.5288e-09, 5.5663e-02, 1.0000e+00, 9.9690e-01,\n",
            "         9.2741e-06, 5.1836e-01, 9.9984e-01, 5.3884e-04],\n",
            "        [3.2747e-03, 2.6509e-04, 2.5490e-12, 4.1583e-02, 9.9857e-01, 1.0000e+00,\n",
            "         2.5571e-03, 6.1294e-01, 1.0000e+00, 1.1006e-04],\n",
            "        [9.6063e-06, 1.1846e-04, 1.2344e-11, 9.6783e-01, 9.6750e-01, 1.0000e+00,\n",
            "         8.2951e-04, 9.7823e-01, 1.0000e+00, 6.9053e-04],\n",
            "        [5.2790e-04, 4.7633e-01, 1.9614e-14, 5.1959e-02, 9.9837e-01, 1.0000e+00,\n",
            "         1.5478e-06, 6.7239e-05, 9.9969e-01, 8.4337e-04],\n",
            "        [3.9499e-07, 9.9958e-01, 1.1357e-10, 8.6151e-01, 9.8513e-01, 9.8816e-01,\n",
            "         8.2546e-03, 4.1545e-02, 1.0000e+00, 9.9758e-04],\n",
            "        [5.0882e-02, 4.4644e-02, 4.3967e-10, 7.8945e-01, 2.4632e-02, 9.9995e-01,\n",
            "         9.7842e-05, 2.7815e-03, 1.0000e+00, 4.8075e-06],\n",
            "        [3.1773e-04, 1.5611e-02, 4.2166e-08, 6.1190e-04, 1.0000e+00, 1.0000e+00,\n",
            "         1.6987e-06, 7.9291e-02, 1.0000e+00, 2.4155e-09],\n",
            "        [1.8027e-06, 9.9942e-01, 8.3321e-10, 6.7581e-01, 9.9998e-01, 1.0000e+00,\n",
            "         5.5954e-07, 3.7649e-06, 1.0000e+00, 1.2916e-06],\n",
            "        [1.0367e-08, 2.4707e-01, 1.7237e-05, 9.9999e-01, 1.0000e+00, 1.0000e+00,\n",
            "         3.9303e-05, 1.0537e-01, 1.0000e+00, 6.8990e-01],\n",
            "        [1.3649e-04, 6.0824e-04, 1.0072e-09, 7.5567e-03, 9.9992e-01, 1.0000e+00,\n",
            "         2.3459e-04, 4.9525e-03, 1.0000e+00, 2.5352e-08],\n",
            "        [5.6345e-07, 2.7452e-04, 6.1377e-13, 9.9202e-01, 9.8983e-01, 9.9999e-01,\n",
            "         2.1740e-02, 1.2344e-02, 1.0000e+00, 9.3610e-03],\n",
            "        [1.8345e-06, 9.0942e-01, 4.6546e-05, 9.9998e-01, 9.9984e-01, 9.9991e-01,\n",
            "         5.0164e-01, 1.4191e-03, 1.0000e+00, 3.5858e-06],\n",
            "        [9.9039e-08, 9.9353e-01, 3.5823e-08, 9.9904e-01, 9.9921e-01, 1.0000e+00,\n",
            "         7.6253e-03, 9.8213e-01, 1.0000e+00, 3.6938e-02],\n",
            "        [1.9967e-06, 2.4819e-01, 1.4259e-09, 9.2343e-01, 9.9934e-01, 1.0000e+00,\n",
            "         1.2155e-02, 8.4038e-01, 1.0000e+00, 2.2043e-06],\n",
            "        [2.4018e-06, 9.9972e-01, 4.0603e-08, 9.9990e-01, 9.9999e-01, 1.0000e+00,\n",
            "         4.5815e-07, 2.5270e-06, 1.0000e+00, 7.6488e-04],\n",
            "        [3.8333e-06, 8.0520e-03, 4.6305e-04, 9.9818e-05, 9.9917e-01, 1.0000e+00,\n",
            "         2.5510e-06, 7.1295e-03, 1.0000e+00, 1.7884e-03],\n",
            "        [2.1753e-01, 9.7738e-01, 1.1199e-07, 7.7934e-02, 9.9888e-01, 9.9999e-01,\n",
            "         9.2575e-03, 2.2357e-01, 1.0000e+00, 6.3653e-05],\n",
            "        [1.9995e-05, 1.8215e-07, 3.5933e-12, 7.8748e-01, 3.9342e-02, 9.9974e-01,\n",
            "         5.7079e-07, 9.4766e-01, 1.0000e+00, 4.6246e-03],\n",
            "        [6.5150e-13, 8.4377e-05, 4.0570e-07, 8.8577e-01, 6.0557e-01, 1.0000e+00,\n",
            "         2.6415e-03, 4.5116e-02, 1.0000e+00, 8.5800e-01],\n",
            "        [1.5186e-08, 6.6874e-01, 6.8903e-09, 9.9974e-01, 9.9925e-01, 1.0000e+00,\n",
            "         1.2759e-05, 4.6694e-03, 1.0000e+00, 1.6452e-04],\n",
            "        [5.1651e-06, 1.6727e-01, 5.0606e-12, 4.3635e-01, 9.5560e-01, 9.9991e-01,\n",
            "         8.4218e-04, 1.5772e-01, 9.9998e-01, 5.9981e-07]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiaTY4NkoMmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_activation(x):\n",
        "  return torch.exp(x) / torch.sum(torch.exp(x), dim = 1).view(-1, 1)    # dim = 1 means to sum across the columns in one row.\n",
        "\n",
        "'''\n",
        "torch.exp(x) is a (64, 10) tensor.\n",
        "whereas, torch.sum(torch.exp(x), dim = 1) is a (64) vector.\n",
        "Thus, to (check how does this calculation works) we reshape the vector by using .view()\n",
        "'''"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NvHzjt-CeCa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "a1d8e98d-5261-49bf-abf5-4d500065f20b"
      },
      "source": [
        "probability = softmax_activation(output)\n",
        "print(probability.shape)\n",
        "print(probability.sum(dim = 1))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCL-4F8gClGl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "23dfb610-7c29-4aa9-cc14-4fa14597e961"
      },
      "source": [
        "# Or apply softmax activation to the output layer only.\n",
        "\n",
        "h1 = sigmoid_activation(torch.mm(inputs, W1) + B1)\n",
        "output = softmax_activation(torch.mm(h1, W2) + B2)\n",
        "print(output.shape)\n",
        "print(output.sum(dim =))\n",
        "# Softmax finds out the probability. Thus, the probabilities across all 10 numbers from 0 to 9 should be 1"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkQrVsbIKEBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}